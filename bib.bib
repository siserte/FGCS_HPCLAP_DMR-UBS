
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@article{tarraf_malleability_2024,
	title = {Malleability in {Modern} {HPC} {Systems}: {Current} {Experiences}, {Challenges}, and {Future} {Opportunities}},
	issn = {1558-2183},
	shorttitle = {Malleability in {Modern} {HPC} {Systems}},
	doi = {10.1109/TPDS.2024.3406764},
	urldate = {2024-05-31},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Tarraf, Ahmad and Schreiber, Martin and Cascajo, Alberto and Besnard, Jean-Baptiste and Vef, Marc-André and Huber, Dominik and Happ, Sonja and Brinkmann, André and Singh, David E. and Hoppe, Hans-Christian and Miranda, Alberto and Peña, Antonio J. and Machado, Rui and Gasulla, Marta Garcia- and Schulz, Martin and Carpenter, Paul and Pickartz, Simon and Rotaru, Tiberiu and Iserte, Sergio and Lopez, Victor and Ejarque, Jorge and Sirwani, Heena and Wolf, Felix},
	month = jun,
	year = {2024},
	keywords = {Dynamic scheduling, HPC, Malleability, Monitoring, Resource management, Runtime, State-of-the-art, Survey, Systems support, Terminology, Throughput},
	pages = {1--14},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\bscuser\\Zotero\\storage\\IA394G28\\10541114.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\bscuser\\Zotero\\storage\\2JDN7KRG\\Tarraf et al. - 2024 - Malleability in Modern HPC Systems Current Experi.pdf:application/pdf},
}

@article{iserte_dmr_2018,
	title = {{DMR} {API}: {Improving} {Cluster} {Productivity} by {Turning} {Applications} into {Malleable}},
	volume = {78},
	issn = {0167-8191},
	doi = {10.1016/j.parco.2018.07.006},
	journal = {Parallel Computing},
	author = {Iserte, S. and Mayo, R. and Quintana-Ortí, E. S. and Beltran, V. and Peña, A. J.},
	month = jul,
	year = {2018},
	pages = {54--66},
}

@article{iserte_dmrlib_2020,
	title = {{DMRlib}: {Easy}-coding and {Efficient} {Resource} {Management} for {Job} {Malleability}},
	volume = {70},
	issn = {0018-9340},
	doi = {10.1109/TC.2020.3022933},
	journal = {IEEE Transactions on Computers},
	author = {Iserte, S. and Mayo, R. and Quintana-Ortí, E. S. and Peña, A. J.},
	month = sep,
	year = {2020},
	pages = {1443--1457},
	file = {Full Text:C\:\\Users\\bscuser\\Zotero\\storage\\5Y9JY488\\Iserte et al. - 2020 - DMRlib Easy-coding and Efficient Resource Managem.pdf:application/pdf},
}

@article{iserte_study_2020,
	title = {A {Study} of the {Effect} of {Process} {Malleability} in the {Energy} {Efficiency} on {GPU}-based {Clusters}},
	volume = {76},
	issn = {0920-8542},
	doi = {10.1007/s11227-019-03034-x},
	journal = {Journal of Supercomputing},
	author = {Iserte, S. and Rojek, K.},
	month = oct,
	year = {2020},
	pages = {255--274},
}

@article{martin-alvarez_proteo_2024,
	title = {Proteo: a framework for the generation and evaluation of malleable {MPI} applications},
	issn = {1573-0484},
	shorttitle = {Proteo},
	doi = {10.1007/s11227-024-06277-5},
	abstract = {Applying malleability to HPC systems can increase their productivity without degrading or even improving the performance of running applications. This paper presents Proteo, a configurable framework that allows to design benchmarks to study the effect of malleability on a system, and also incorporates malleability into a real application. Proteo consists of two modules: SAM allows to emulate the computational behavior of iterative scientific MPI applications, and MaM is able to reconfigure a job during execution, adjusting the number of processes, redistributing data, and resuming execution. An in-depth study of all the possibilities shows that Proteo is able to behave like a real malleable or non-malleable application in the range [0.85, 1.15]. Furthermore, the different methods defined in MaM for process management and data redistribution are analyzed, concluding that asynchronous malleability, where reconfiguration and application execution overlap, results in a 1.15\{\vphantom{\}}{\textbackslash}textbackslashtimes{\textbackslash}speedup.},
	language = {en},
	urldate = {2024-07-03},
	journal = {The Journal of Supercomputing},
	author = {Martín-Álvarez, Iker and Aliaga, José I. and Castillo, Maribel and Iserte, Sergio},
	month = jul,
	year = {2024},
	keywords = {Dynamic resources, Emulation, Malleability, MPI},
	file = {Full Text PDF:C\:\\Users\\bscuser\\Zotero\\storage\\WUGNJYYH\\Martín-Álvarez et al. - 2024 - Proteo a framework for the generation and evaluat.pdf:application/pdf},
}

@inproceedings{iserte_parallel_2024,
	address = {Madrid, Spain},
	title = {Parallel {Efficiency}-aware {Standard} {MPI}-based {Malleability}},
	booktitle = {Euro-par {Workshops} {Proceedings}},
	author = {Iserte, Sergio and Lopez, Victor and Garcia-Gasulla, Marta and Peña, Antonio J.},
	month = aug,
	year = {2024},
}

@inproceedings{iserte_towards_2025,
	address = {Cham},
	title = {Towards the {Democratization} and {Standardization} of {Dynamic} {Resources} with {MPI} {Spawning}},
	isbn = {978-3-031-85697-6},
	doi = {10.1007/978-3-031-85697-6_19},
	abstract = {This paper presents an efficient tool for managing dynamic resources in production high-performance computing (HPC) settings, focusing on flexibility, adaptability, and user-friendliness. We introduce a unified dynamic resource management application programming interface (API) that supports a wide range of HPC applications, allowing seamless integration without direct interaction with Dynamic Management of Resources (DMR). The DMR framework, evolved from the DMRlib structure, now supports various dynamic resource managers and includes the Proteo reconfiguration engine to enhance malleability strategies. This integration addresses previous limitations by allowing diverse reconfiguration methods without respawning all processes or lacking RMS support. The paper also showcases the solution’s performance and coding productivity with the MPDATA (Multidimensional Positive Definite Advection Transport Algorithm) application. Key contributions include an enhanced modular DMR framework supporting different reconfiguration managers, upgraded DMRlib with the Proteo reconfiguration engine, offering extensive reconfiguration strategies, and a malleable version of the MPDATA solver.},
	language = {en},
	booktitle = {Parallel {Processing} and {Applied} {Mathematics}},
	publisher = {Springer Nature Switzerland},
	author = {Iserte, Sergio and Martín-Álvarez, Iker and Rojek, Krzysztof and Aliaga, José I. and Castillo, Maribel and Peña, Antonio J.},
	editor = {Wyrzykowski, Roman and Dongarra, Jack and Deelman, Ewa and Karczewski, Konrad},
	year = {2025},
	pages = {287--300},
}

@article{iserte_resource_2025,
	title = {Resource optimization with {MPI} process malleability for dynamic workloads in {HPC} clusters},
	issn = {0167-739X},
	doi = {10.1016/j.future.2025.107949},
	abstract = {Dynamic resource management is essential for optimizing computational efficiency in modern high-performance computing (HPC) environments, particularly as systems scale. While research has demonstrated the benefits of malleability in resource management systems (RMS), the adoption of such techniques in production environments remains limited due to challenges in standardization, interoperability, and usability. Addressing these gaps, this paper extends our prior work on the Dynamic Management of Resources (DMR) framework, which provides a modular and user-friendly approach to dynamic resource allocation. Building upon the original DMRlib reconfiguration runtime, this work integrates new methodology from the Malleability Module (MaM) of the Proteo framework, further enhancing reconfiguration capabilities with new spawning strategies and data redistribution methods. In this paper, we explore new malleability strategies in HPC dynamic workloads, such as merging MPI communicators and asynchronous reconfigurations, which offer new opportunities for dramatically reducing memory overhead. The proposed enhancements are rigorously evaluated on a world-class supercomputer, demonstrating improved resource utilization and workload efficiency. Results show that dynamic resource management can reduce the workload completion time by 40\% and increase the resource utilization by over 20\%, compared to static resource allocation.},
	urldate = {2025-06-14},
	journal = {Future Generation Computer Systems},
	author = {Iserte, Sergio and Martín-Álvarez, Iker and Rojek, Krzysztof and Aliaga, José I. and Castillo, Maribel and Folwarska, Weronika and Peña, Antonio J.},
	month = jun,
	year = {2025},
	keywords = {Dynamic workloads, High-performance computing, Job reconfiguration, MPI malleability, Resource management},
	pages = {107949},
	file = {ScienceDirect Snapshot:C\:\\Users\\bscuser\\Zotero\\storage\\Q9Y54VFK\\S0167739X25002444.html:text/html},
}

@inproceedings{huber_bridging_2025,
	title = {Bridging the {Gap} {Between} {Genericity} and {Programmability} of {Dynamic} {Resources} in {HPC}},
	abstract = {Dynamic Resource Management (DRM) allows dynamically changing the resources assigned to a job during its execution. This flexibility can improve productivity in completed jobs per unit of time, resource utilization rate, and energy consumption, among other metrics. Despite its benefits, DRM remains complex to implement, which is one of the reasons it has not yet been widely adopted in production HPC systems. To address this challenge, we design, implement, and evaluate a methodology aimed at improving the programmability of DRM for iterative algorithm-based applications, while maintaining a generic and flexible foundation. Specifically, we interface the Dynamic Management of Resources API (DMR-API), an application-level abstraction layer that simplifies the adoption of dynamic resources—particularly in classical iteration-based HPC applications—with the Dynamic Processes with PSets (DPP) approach, which provides a set of generic design principles for dynamic resource management in high-performance parallel programming models. This integration makes DRM more accessible for iteration-based HPC applications through the DMR-API, while preserving the generality and flexibility of DPP at the system software level to support a wider range of HPC application types. Our results show that DRM can be effectively leveraged in HPC environments with minimal coding effort, unlocking the benefits of dynamic resource allocation for job throughput and system utilization.},
	urldate = {2025-06-14},
	booktitle = {{ISC} {High} {Performance} 2025 {Research} {Paper} {Proceedings} (40th {International} {Conference})},
	author = {Huber, Dominik and Iserte, Sergio and Schreiber, Martin and Peña, Antonio J. and Schulz, Martin},
	month = jun,
	year = {2025},
	keywords = {Benchmark testing, Dynamic programming, Dynamic scheduling, Energy consumption, Production systems, Productivity, Resource management, Runtime, System software, Throughput},
	pages = {1--11},
	file = {Full Text PDF:C\:\\Users\\bscuser\\Zotero\\storage\\2V7M484P\\Huber et al. - 2025 - Bridging the Gap Between Genericity and Programmab.pdf:application/pdf},
}

@article{aliaga_survey_2022,
	title = {A {Survey} on {Malleability} {Solutions} for {High}-{Performance} {Distributed} {Computing}},
	volume = {12},
	issn = {2076-3417},
	doi = {10.3390/app12105231},
	journal = {Applied Science},
	author = {Aliaga, J. I. and Castillo, M. and Iserte, S. and Martín-Álvarez, I. and Mayo, R.},
	month = may,
	year = {2022},
	pages = {1--32},
	file = {Full Text:C\:\\Users\\bscuser\\Zotero\\storage\\6XKTXNCW\\Aliaga et al. - 2022 - A Survey on Malleability Solutions for High-Perfor.pdf:application/pdf},
}

@inproceedings{zakay_preserving_2014,
	title = {Preserving {User} {Behavior} {Characteristics} in {Trace}-{Based} {Simulation} of {Parallel} {Job} {Scheduling}},
	doi = {10.1109/MASCOTS.2014.15},
	abstract = {Evaluating the performance of a computer system requires the use of representative workloads. Therefore it is customary to use recorded job traces in simulations to evaluate the performance of proposed parallel job schedulers. We argue that this practice retains unimportant attributes of the workload, at the expense of other more important attributes. Specifically, using traces in open-system simulations retains the exact timestamps at which jobs are submitted. But in a real system these times depend on how users react to the performance of previous jobs, and it is more important to preserve the logical structure of dependencies between jobs than the specific timestamps. Using dependency information extracted from traces, we show how a simulation can preserve these dependencies. To do so we also extract user behavior, in terms of sessions and think times between the termination of one batch of jobs and the submission of a subsequent batch.},
	urldate = {2025-07-01},
	booktitle = {2014 {IEEE} 22nd {International} {Symposium} on {Modelling}, {Analysis} \& {Simulation} of {Computer} and {Telecommunication} {Systems}},
	author = {Zakay, Netanel and Feitelson, Dror G.},
	month = sep,
	year = {2014},
	keywords = {Analytical models, Computational modeling, Feedback, Load modeling, Measurement, Reliability, Simulation, Throughput, Time factors, Workload trace},
	pages = {51--60},
	file = {Snapshot:C\:\\Users\\bscuser\\Zotero\\storage\\E7V9SQIQ\\7033637.html:text/html},
}

@inproceedings{schroeder_open_2006,
	address = {USA},
	series = {{NSDI}'06},
	title = {Open versus closed: a cautionary tale},
	shorttitle = {Open versus closed},
	abstract = {Workload generators may be classified as based on a closed system model, where new job arrivals are only triggered by job completions (followed by think time), or an open system model, where new jobs arrive independently of job completions. In general, system designers pay little attention to whether a workload generator is closed or open.Using a combination of implementation and simulation experiments, we illustrate that there is a vast difference in behavior between open and closed models in real-world settings. We synthesize these differences into eight simple guiding principles, which serve three purposes. First, the principles specify how scheduling policies are impacted by closed and open models, and explain the differences in user level performance. Second, the principles motivate the use of partly open system models, whose behavior we show to lie between that of closed and open models. Finally, the principles provide guidelines to system designers for determining which system model is most appropriate for a given workload.},
	urldate = {2025-07-01},
	booktitle = {Proceedings of the 3rd conference on {Networked} {Systems} {Design} \& {Implementation} - {Volume} 3},
	publisher = {USENIX Association},
	author = {Schroeder, Bianca and Wierman, Adam and Harchol-Balter, Mor},
	month = may,
	year = {2006},
	pages = {18},
}

@inproceedings{schlagkamp_influence_2017,
	title = {Influence of {Dynamic} {Think} {Times} on {Parallel} {Job} {Scheduler} {Performances} in {Generative} {Simulations}},
	doi = {10.1007/978-3-319-61756-5_7},
	abstract = {The performance of parallel schedulers is a crucial factor in the efficiency of high performance computing environments. Scheduler designs for practical application focusing on improving certain metrics can only be achieved, if they are evaluated in realistic testing...},
	language = {en},
	urldate = {2025-07-01},
	booktitle = {3rd conference on {Networked} {Systems} {Design} \& {Implementation}},
	author = {Schlagkamp, Stephan},
	month = jul,
	year = {2017},
}

@inproceedings{zakay_identifying_2013,
	title = {On {Identifying} {User} {Session} {Boundaries} in {Parallel} {Workload} {Logs}},
	doi = {10.1007/978-3-642-35867-8_12},
	abstract = {The stream of jobs submitted to a parallel supercomputer is actually the interleaving of many streams from different users, each of which is composed of sessions. Identifying and characterizing the sessions is important in the context of workload modeling, especially...},
	language = {en},
	urldate = {2025-07-01},
	booktitle = {Workshop on {Job} {Scheduling} {Strategies} for {Parallel} {Processing}},
	author = {Zakay, Netanel and Feitelson, Dror G.},
	year = {2013},
}

@article{madon_replay_2024,
	title = {Replay with {Feedback}: : {How} does the performance of {HPC} system impact user submission behavior?},
	volume = {155},
	doi = {10.1016/j.future.2024.01.024},
	number = {C},
	urldate = {2025-07-01},
	journal = {Future Generation Computer Systems},
	author = {Madon, Maël and Da Costa, Georges and Pierson, Jean-Marc},
	month = jun,
	year = {2024},
	keywords = {HPC simulation, Parallel workload, Performance evaluation, Reproducibility, Scheduling, User behavior},
	pages = {66--79},
}

@inproceedings{feitelson_resampling_2021,
	title = {Resampling with {Feedback}: {A} {New} {Paradigm} of {Using} {Workload} {Data} for {Performance} {Evaluation}},
	shorttitle = {Resampling with {Feedback}},
	doi = {10.1007/978-3-030-88224-2_1},
	abstract = {Reliable performance evaluations require representative workloads. This has led to the use of accounting logs from production systems as a source for workload data in simulations. But using such logs directly suffers from various deficiencies, such as providing data...},
	language = {en},
	urldate = {2025-07-01},
	booktitle = {Workshop on {Job} {Scheduling} {Strategies} for {Parallel} {Processing}},
	author = {Feitelson, Dror G.},
	month = oct,
	year = {2021},
}

@inproceedings{feitelson_packing_1996,
	address = {Berlin, Heidelberg},
	series = {{IPPS} '96},
	title = {Packing {Schemes} for {Gang} {Scheduling}},
	isbn = {978-3-540-61864-5},
	urldate = {2025-07-01},
	booktitle = {Proceedings of the {Workshop} on {Job} {Scheduling} {Strategies} for {Parallel} {Processing}},
	publisher = {Springer-Verlag},
	author = {Feitelson, Dror G.},
	month = apr,
	year = {1996},
	pages = {89--110},
}

@article{lublin_workload_2003,
	title = {The workload on parallel supercomputers: modeling the characteristics of rigid jobs},
	volume = {63},
	issn = {0743-7315},
	shorttitle = {The workload on parallel supercomputers},
	doi = {10.1016/S0743-7315(03)00108-4},
	abstract = {The analysis of workloads is important for understanding how systems are used. In addition, workload models are needed as input for the evaluation of new system designs, and for the comparison of system designs. This is especially important in costly large-scale parallel systems. Luckily, workload data are available in the form of accounting logs. Using such logs from three different sites, we analyze and model the job-level workloads with an emphasis on those aspects that are universal to all sites. As many distributions turn out to span a large range, we typically first apply a logarithmic transformation to the data, and then fit it to a novel hyper-Gamma distribution or one of its special cases. This is a generalization of distributions proposed previously, and leads to good goodness-of-fit scores. The parameters for the distribution are found using the iterative EM algorithm. The results of the analysis have been codified in a modeling program that creates a synthetic workload based on the results of the analysis.},
	number = {11},
	urldate = {2025-07-01},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Lublin, Uri and Feitelson, Dror G.},
	month = nov,
	year = {2003},
	keywords = {Arrival pattern, Parallel jobs, Runtime distribution, Size distribution, Workload model},
	pages = {1105--1122},
	file = {ScienceDirect Snapshot:C\:\\Users\\bscuser\\Zotero\\storage\\QD93H9HU\\S0743731503001084.html:text/html},
}

@inproceedings{sudarsan_reshape_2007,
	title = {{ReSHAPE}: {A} {Framework} for {Dynamic} {Resizing} and {Scheduling} of {Homogeneous} {Applications} in a {Parallel} {Environment}},
	shorttitle = {{ReSHAPE}},
	doi = {10.1109/ICPP.2007.73},
	abstract = {A traditional application scheduler running on a parallel cluster only supports static scheduling where the number of processors allocated to an application remains fixed throughout the lifetime of the job. Due to unpredictability in job arrival times and varying resource requirements, static scheduling can result in idle system resources thereby decreasing the overall system throughput. In this paper we present a prototype framework called ReSHAPE, which supports dynamic resizing of parallel MPI applications executed on distributed memory platforms. The framework includes a scheduler that supports resizing of applications, an API to enable applications to interact with the scheduler, and a library that makes resizing viable. Applications executed using the ReSHAPE scheduler framework can expand to take advantage of additional free processors or can shrink to accommodate a high priority application, without getting suspended. Experimental results show that the ReSHAPE framework can improve individual job turn-around time and overall system throughput.},
	urldate = {2025-07-01},
	booktitle = {2007 {International} {Conference} on {Parallel} {Processing} ({ICPP} 2007)},
	author = {Sudarsan, Rajesh and Ribbens, Calvin J.},
	month = sep,
	year = {2007},
	issn = {ISSN: 2332-5690},
	keywords = {Application software, Computer science, Dynamic scheduling, Processor scheduling, Prototypes, Quality of service, Resource management, Runtime library, Supercomputers, Throughput},
	pages = {44--44},
	file = {Snapshot:C\:\\Users\\bscuser\\Zotero\\storage\\T3VEDXWD\\4343851.html:text/html;Submitted Version:C\:\\Users\\bscuser\\Zotero\\storage\\RGRGU5UL\\Sudarsan and Ribbens - 2007 - ReSHAPE A Framework for Dynamic Resizing and Sche.pdf:application/pdf},
}

@inproceedings{sudarsan_dynamic_2009,
	address = {Berlin, Heidelberg},
	series = {{ICCS} '09},
	title = {Dynamic {Resizing} of {Parallel} {Scientific} {Simulations}: {A} {Case} {Study} {Using} {LAMMPS}},
	isbn = {978-3-642-01969-2},
	shorttitle = {Dynamic {Resizing} of {Parallel} {Scientific} {Simulations}},
	doi = {10.1007/978-3-642-01970-8_18},
	abstract = {Large-scale computational science simulations are a dominant component of the workload on modern supercomputers. Efficient use of high-end resources for these large computations is of considerable scientific and economic importance. However, conventional job schedulers limit flexibility in that they are `static', i.e., the number of processors allocated to an application can not be changed at runtime. In earlier work, we described ReSHAPE, a system that eliminates this drawback by supporting dynamic resizability in distributed-memory parallel applications. The goal of this paper is to present a case study highlighting the steps involved in adapting a production scientific simulation code to take advantage of ReSHAPE. LAMMPS, a widely used molecular dynamics code, is the test case. Minor extensions to LAMMPS allow it to be resized using ReSHAPE, and experimental results show that resizing significantly improves overall system utilization as well as performance of an individual LAMMPS job.},
	urldate = {2025-07-01},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Computational} {Science}: {Part} {I}},
	publisher = {Springer-Verlag},
	author = {Sudarsan, Rajesh and Ribbens, Calvin J. and Farkas, Diana},
	month = may,
	year = {2009},
	pages = {175--184},
	file = {Full Text:C\:\\Users\\bscuser\\Zotero\\storage\\MIY4TTXC\\Sudarsan et al. - 2009 - Dynamic Resizing of Parallel Scientific Simulation.pdf:application/pdf},
}

@inproceedings{sarood_maximizing_2014,
	address = {New Orleans, Louisana},
	series = {{SC} '14},
	title = {Maximizing throughput of overprovisioned {HPC} data centers under a strict power budget},
	isbn = {978-1-4799-5500-8},
	doi = {10.1109/SC.2014.71},
	abstract = {Building future generation supercomputers while constraining their power consumption is one of the biggest challenges faced by the HPC community. For example, US Department of Energy has set a goal of 20 MW for an exascale (1018 flops) supercomputer. To realize this goal, a lot of research is being done to revolutionize hardware design to build power efficient computers and network interconnects. In this work, we propose a software-based online resource management system that leverages hardware facilitated capability to constrain the power consumption of each node in order to optimally allocate power and nodes to a job. Our scheme uses this hardware capability in conjunction with an adaptive runtime system that can dynamically change the resource configuration of a running job allowing our resource manager to re-optimize allocation decisions to running jobs as new jobs arrive, or a running job terminates.We also propose a performance modeling scheme that estimates the essential power characteristics of a job at any scale. The proposed online resource manager uses these performance characteristics for making scheduling and resource allocation decisions that maximize the job throughput of the supercomputer under a given power budget. We demonstrate the benefits of our approach by using a mix of jobs with different power-response characteristics. We show that with a power budget of 4.75 MW, we can obtain up to 5.2X improvement in job throughput when compared with the SLURM scheduling policy that is power-unaware. We corroborate our results with real experiments on a relatively small scale cluster, in which we obtain a 1.7X improvement.},
	urldate = {2025-07-01},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE Press},
	author = {Sarood, Osman and Langer, Akhil and Gupta, Abhishek and Kale, Laxmikant},
	month = nov,
	year = {2014},
	pages = {807--818},
}

@inproceedings{prabhakaran_batch_2015,
	address = {USA},
	series = {{IPDPS} '15},
	title = {A {Batch} {System} with {Efficient} {Adaptive} {Scheduling} for {Malleable} and {Evolving} {Applications}},
	isbn = {978-1-4799-8649-1},
	doi = {10.1109/IPDPS.2015.34},
	abstract = {The throughput of supercomputers depends not only on efficient job scheduling but also on the type of jobs that form the workload. Malleable jobs are most favourable for a cluster as they can dynamically adapt to a changing allocation of resources. The batch system can expand or shrink a running malleable job to improve system utilization, throughput, and response times. In the past, however, the rigid nature of commonly used programming models like MPI made writing malleable applications a daunting task, which is why it remained largely unrealized. This is now changing. To improve fault tolerance, load imbalance, and energy efficiency in emerging exactable systems, more adaptive programming paradigms such as Charm++ enter the scene. Although they offer better support for malleability, current batch systems still lack management facilities for malleable jobs and are therefore incapable of leveraging their potential. In this paper, we present an extension of the Torque/Maui batch system for malleability. We propose a novel malleable job scheduling strategy and show the first batch system capable of efficiently managing rigid, malleable, and evolving jobs together. We demonstrate that our strategy achieves consistently superior performance in comparison to every other state-of-the-art malleable job scheduling strategy under varying dynamics of the workload.},
	urldate = {2025-07-01},
	booktitle = {Proceedings of the 2015 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium}},
	publisher = {IEEE Computer Society},
	author = {Prabhakaran, Suraj and Neumann, Marcel and Rinke, Sebastian and Wolf, Felix and Gupta, Abhishek and Kale, Laxmikant V.},
	month = may,
	year = {2015},
	pages = {429--438},
}

@article{iserte_dynamic_2019,
	title = {Dynamic reconfiguration of noniterative scientific applications: {A} case study with {HPG} aligner},
	volume = {33},
	issn = {1094-3420},
	shorttitle = {Dynamic reconfiguration of noniterative scientific applications},
	doi = {10.1177/1094342018802347},
	abstract = {Several studies have proved the benefits of job malleability, that is, the capacity of an application to adapt its parallelism to a dynamically changing number of allocated processors. The most remarkable advantages of executing malleable jobs as part of a high performance computer workload are the throughput increase and the more efficient utilization of the underlying resources. Malleability has been mostly applied to iterative applications where all the processes execute the same operations over different sets of data and with a balanced per process load. Unfortunately, not all scientific applications adhere to this process-level malleable job structure. There are scientific applications which are either noniterative or present an irregular per process load distribution. Unlike many other reconfiguration tools, the Dynamic Management of Resources Application Programming Interface (DMR API) provides the necessary flexibility to make malleable these out-of-target applications. In this article, we study the particular case of using the DMR API to generate a malleable version of HPG aligner, a distributed-memory noniterative genomic sequencer featuring an irregular communication pattern among processes. Through this first conversion of an out-of-target application to a malleable job, we both illustrate how the DMR API may be used to convert this type of applications into malleable and test the benefits of this conversion in production clusters. Our experimental results reveal an important reduction of the malleable HPG aligner jobs completion time compared to the original HPG aligner version. Furthermore, HPG aligner malleable workloads achieve a greater throughput than their fixed counterparts.},
	language = {EN},
	number = {5},
	urldate = {2025-07-01},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Iserte, Sergio and Martínez, Héctor and Barrachina, Sergio and Castillo, Maribel and Mayo, Rafael and Peña, Antonio J},
	month = sep,
	year = {2019},
	pages = {804--816},
	file = {Submitted Version:C\:\\Users\\bscuser\\Zotero\\storage\\VYEEN35E\\Iserte et al. - 2019 - Dynamic reconfiguration of noniterative scientific.pdf:application/pdf},
}

@inproceedings{feitelson_towards_1996,
	address = {Berlin, Heidelberg},
	series = {{IPPS} '96},
	title = {Towards {Convergence} in {Job} {Schedulers} for {Parallel} {Supercomputers}},
	isbn = {978-3-540-61864-5},
	urldate = {2025-07-01},
	booktitle = {Proceedings of the {Workshop} on {Job} {Scheduling} {Strategies} for {Parallel} {Processing}},
	publisher = {Springer-Verlag},
	author = {Feitelson, Dror G. and Rudolph, Larry},
	month = apr,
	year = {1996},
	pages = {1--26},
}

@inproceedings{lopez_talp_2021,
	address = {New York, NY, USA},
	series = {{PERMAVOST} '21},
	title = {{TALP}: {A} {Lightweight} {Tool} to {Unveil} {Parallel} {Efficiency} of {Large}-scale {Executions}},
	isbn = {978-1-4503-8387-5},
	shorttitle = {{TALP}},
	doi = {10.1145/3452412.3462753},
	abstract = {This paper presents the design, implementation, and application of TALP, a lightweight, portable, extensible, and scalable tool for online parallel performance measurement. The efficiency metrics reported by TALP allow HPC users to evaluate the parallel efficiency of their executions, both post-mortem and at runtime. The API that TALP provides allows the running application or resource managers to collect performance metrics at runtime. This enables the opportunity to adapt the execution based on the metrics collected dynamically. The set of metrics collected by TALP are well defined, independent of the tool, and consolidated. We extend the collection of metrics with two additional ones that can differentiate between the load imbalance originated from the intranode or internode imbalance. We evaluate the potential of TALP with three parallel applications that present various parallel issues and carefully analyze the overhead introduced to determine its limitations.},
	urldate = {2025-07-01},
	booktitle = {Proceedings of the 2021 on {Performance} {EngineeRing}, {Modelling}, {Analysis}, and {VisualizatiOn} {STrategy}},
	publisher = {Association for Computing Machinery},
	author = {Lopez, Victor and Ramirez Miranda, Guillem and Garcia-Gasulla, Marta},
	month = jun,
	year = {2021},
	pages = {3--10},
	file = {Full Text:C\:\\Users\\bscuser\\Zotero\\storage\\Z3GWV7VZ\\Lopez et al. - 2021 - TALP A Lightweight Tool to Unveil Parallel Effici.pdf:application/pdf},
}

@incollection{rojek_parallelization_2015,
	series = {Guide {Proceedings}},
	title = {Parallelization of {3D} {MPDATA} {Algorithm} {Using} {Many} {Graphics} {Processors}},
	isbn = {978-3-319-21908-0},
	abstract = {EULAG Eulerian/semi-Lagrangian fluid solver is an established numerical model for simulating thermo-fluid flows across a wide range of scales and physical scenarios. The multidimensional positive definite advection transport algorithm MPDATA is among the most time-consuming components of EULAG. In this study, we focus on adapting the 3D MPDATA computations to clusters with graphics processors. Our approach is based on a hierarchical decomposition including the level of cluster, as well as an optimized distribution of computations between GPU resources within each node. To implement the resulting computing scheme, the MPI standard is used across nodes, while CUDA is applied inside nodes. We present performance results for the 3D MPDATA code running on the NVIDIA GeForce GTX TITAN graphics card, as well as on the Piz Daint cluster equipped with NVIDIA Tesla K20x GPUs. In particular, the sustained performance of 138 Gflop/s is achieved for a single GPU, which scales upï źto more than 11 Tflop/s for 256 GPUs.},
	urldate = {2025-07-01},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Parallel} {Computing} {Technologies} - {Volume} 9251},
	author = {Rojek, Krzysztof and and Wyrzykowski, Roman},
	month = aug,
	year = {2015},
	doi = {10.1007/978-3-319-21909-7_43},
	keywords = {Algorithm adaptation, Cluster, CUDA, GPU, MPDATA algorithm, MPI, Parallel programming, Stencil computations},
	pages = {445--457},
}

@article{vazquez_alya_2016,
	title = {Alya: {Multiphysics} engineering simulation toward exascale},
	volume = {14},
	issn = {1877-7503},
	doi = {10.1016/j.jocs.2015.12.007},
	journal = {Journal of Computational Science},
	author = {Vázquez, Mariano and Houzeaux, Guillaume and Koric, Seid and Artigues, Antoni and Aguado-Sierra, Jazmin and Arís, Ruth and Mira, Daniel and Calmet, Hadrien and Cucchietti, Fernando and Owen, Herbert and Taha, Ahmed and Burness, Evan Dering and Cela, José María and Valero, Mateo},
	year = {2016},
	keywords = {Computational mechanics, Multi-physics coupling, Parallelization},
	pages = {15--27},
}

@article{caviedes-voullieme_serghei_2023,
	title = {{SERGHEI} ({SERGHEI}-{SWE}) v1.0: a performance-portable high-performance parallel-computing shallow-water solver for hydrology and environmental hydraulics},
	volume = {16},
	doi = {10.5194/gmd-16-977-2023},
	number = {3},
	journal = {Geoscientific Model Development},
	author = {Caviedes-Voullième, D. and Morales-Hernández, M. and Norman, M. R. and Özgen-Xian, I.},
	year = {2023},
	pages = {977--1008},
}

@article{godoy_large_2024,
	title = {Large {Language} {Model} {Evaluation} for {High}-{Performance} {Computing} {Software} {Development}},
	volume = {36},
	doi = {10.1002/cpe.8269},
	number = {26},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Godoy, William F. and Valero-Lara, Pedro and Teranishi, Keita and Balaprakash, Prasanna and Vetter, Jeffrey S.},
	year = {2024},
	keywords = {high-performance computing, code generation, auto-parallelization, GPT, large language model, programming models},
	pages = {e8269},
}

@article{rosciszewski_optimizing_2023,
	title = {Optimizing {Throughput} of {Seq2Seq} {Model} {Training} on the {IPU} {Platform} for {AI}-accelerated {CFD} {Simulations}},
	volume = {143},
	doi = {10.1016/j.future.2023.05.004},
	journal = {Future Generation Computer Systems},
	author = {Rosciszewski, P. and Krzywaniak, A. and Iserte, S. and Rojek, K. and Gepner, P.},
	month = may,
	year = {2023},
	issn = {ISBN: 0167-739X},
	pages = {149--162},
}


@article{hess_gromacs_2008,
	title = {{GROMACS} 4:  {Algorithms} for {Highly} {Efficient}, {Load}-{Balanced}, and {Scalable} {Molecular} {Simulation}},
	volume = {4},
	issn = {1549-9618},
	doi = {10.1021/ct700301q},
	abstract = {Molecular simulation is an extremely useful, but computationally very expensive tool for studies of chemical and biomolecular systems. Here, we present a new implementation of our molecular simulation toolkit GROMACS which now both achieves extremely high performance on single processors from algorithmic optimizations and hand-coded routines and simultaneously scales very well on parallel machines. The code encompasses a minimal-communication domain decomposition algorithm, full dynamic load balancing, a state-of-the-art parallel constraint solver, and efficient virtual site algorithms that allow removal of hydrogen atom degrees of freedom to enable integration time steps up to 5 fs for atomistic simulations also in parallel. To improve the scaling properties of the common particle mesh Ewald electrostatics algorithms, we have in addition used a Multiple-Program, Multiple-Data approach, with separate node domains responsible for direct and reciprocal space interactions. Not only does this combination of algorithms enable extremely long simulations of large systems but also it provides that simulation performance on quite modest numbers of standard cluster nodes.},
	number = {3},
	urldate = {2025-07-02},
	journal = {Journal of Chemical Theory and Computation},
	author = {Hess, Berk and Kutzner, Carsten and van der Spoel, David and Lindahl, Erik},
	month = mar,
	year = {2008},
	pages = {435--447},
	file = {Full Text:C\:\\Users\\bscuser\\Zotero\\storage\\KI479DLE\\Hess et al. - 2008 - GROMACS 4  Algorithms for Highly Efficient, Load-.pdf:application/pdf},
}

@inproceedings{martinez_dynamic_2013,
	address = {New York, NY, USA},
	series = {{EuroMPI} '13},
	title = {A dynamic pipeline for {RNA} sequencing on multicore processors},
	isbn = {978-1-4503-1903-4},
	doi = {10.1145/2488551.2488581},
	abstract = {We present a concurrent algorithm for mapping short and long RNA sequences on multicore processors. Our solution processes the data, initially stored on disk, in batches of reads which are passed between the consecutive stages of a pipeline. A major operational reorganization of the original static pipeline, combined with a complete reimplementation based on POSIX threads, renders a dissociated execution between threads and stages/task types, so that threads can compute any type of pending task resulting in a dynamic pipeline. The experiments on a multicore platform reveal that this reorganization yields significantly higher performance, specially for architectures equipped with a small to moderate number of cores.As an additional contribution, our experiments also reveal that the use of 16-nucleotide (nt) seeds during the one of the stages of the pipeline, instead of the 15-nt length that was proposed originally, yields a remarkable reduction in the execution time of the global alignment process while maintaining the sensitivity of the algorithm.},
	urldate = {2025-07-01},
	booktitle = {Proceedings of the 20th {European} {MPI} {Users}' {Group} {Meeting}},
	publisher = {Association for Computing Machinery},
	author = {Martínez, Héctor and Tárraga, Joaquín and Medina, Ignacio and Barrachina, Sergio and Castillo, Maribel and Dopazo, Joaquín and Quintana-Ortí, Enrique S.},
	month = sep,
	year = {2013},
	pages = {235--240},
}

@article{zhong_gpu_2025,
	title = {{GPU} acceleration for {DNA} sequence alignment algorithm and its application},
	volume = {7},
	copyright = {2025 China Computer Federation (CCF)},
	issn = {2524-4930},
	doi = {10.1007/s42514-024-00203-0},
	abstract = {With the rapid development of Next-Generation Sequencing (NGS) technology, genome sequencing services for clinical fields are now bringing new challenges to existing solutions. The increasing demand for alignment data processing motivates the development of more efficient algorithms for computational genomics. The Pair-Hidden Markov Model (Pair-HMM) is one of the most popular models used to process sequence alignment. Its related Forward Algorithm (FA) is usually the key performance bottleneck of the entire variant calling workflow. While multiple previous works have been conducted in efforts to accelerate the algorithm with various levels of parallelization, it still lacks of fully utilizing the resources of heterogeneous devices, such as high-bandwidth memory and massive SIMD cores in advanced GPU. In this paper, we design a GPU-based Pari-HMM sequence alignment algorithm and conduct its implementation with holistic co-design optimizations, including efficient computational parallelization, parameter initialization, memory accessing layout, and etc. When using Nvidia Telsa V100 GPU, Our work has shown speedups of 1151x compared to the Java baseline on Intel single-core CPU and 1.47x to the previous state-of-art GPU work.},
	language = {En},
	number = {2},
	urldate = {2025-07-02},
	journal = {CCF Transactions on High Performance Computing},
	author = {Zhong, Heming and Pan, Xiaojian and He, Zengquang and Wang, Haoling and Huang, Dan and Chen, Zhiguang},
	month = feb,
	year = {2025},
	pages = {169--177},
}

@article{martinez-cuenca_use_2023,
	title = {On the {Use} of {Deep} {Learning} and {Computational} {Fluid} {Dynamics} for the {Estimation} of {Uniform} {Momentum} {Source} {Components} of {Propellers}},
	volume = {26},
	doi = {10.1016/j.isci.2023.108297},
	journal = {iScience},
	author = {Martínez-Cuenca, R. and Luis-Gómez, J. and Iserte, S. and Chiva, S.},
	month = oct,
	year = {2023},
	pages = {1--14},
}

@inproceedings{bungartz_invasive_2013,
	address = {New York, NY, USA},
	series = {X10 '13},
	title = {Invasive computing in {HPC} with {X10}},
	doi = {10.1145/2481268.2481274},
	booktitle = {Proceedings of the third {ACM} {SIGPLAN} {X10} {Workshop}},
	publisher = {ACM},
	author = {Bungartz, Hans-Joachim and Riesinger, Christoph and Schreiber, Martin and Snelting, Gregor and Zwinkau, Andreas},
	year = {2013},
	pages = {12--19},
}

@article{garcia_hints_2014,
	title = {Hints to improve automatic load balancing with {LeWI} for hybrid applications},
	volume = {74},
	issn = {0743-7315},
	doi = {10.1016/j.jpdc.2014.05.004},
	abstract = {The DLB (Dynamic Load Balancing) library and LeWI (LEnd When Idle) algorithm provide a runtime solution to deal with the load imbalance of parallel applications independently of the source of imbalance. DLB relies on the usage of hybrid programming models and exploits the malleability of the second level of parallelism to redistribute computation power across processes. When executing real applications with LeWI, although application’s performance is significantly improved, we have observed in some cases efficiency values between 60\% and 70\%, far from our theoretical limit. This work is a deep analysis of the sources of efficiency loss correlated with application characteristics, parallelization schemes and programming models. We have based our analysis in fine grain monitoring tools and metrics and validated our conclusions by reproducing them in synthetic experiments. As a result, this work teaches us some lessons that can be seen as hints to programmers to help LeWI make an efficient use of computational resources and obtain the maximum performance.},
	number = {9},
	urldate = {2025-07-02},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Garcia, Marta and Labarta, Jesus and Corbalan, Julita},
	month = sep,
	year = {2014},
	keywords = {Application performance analysis, Hybrid parallel programming, Load balancing},
	pages = {2781--2794},
	file = {ScienceDirect Snapshot:C\:\\Users\\bscuser\\Zotero\\storage\\B5C8PBVP\\S0743731514000926.html:text/html},
}

@inproceedings{lopez_openmp_2021,
	address = {Berlin, Heidelberg},
	title = {An {OpenMP} {Free} {Agent} {Threads} {Implementation}},
	isbn = {978-3-030-85261-0},
	doi = {10.1007/978-3-030-85262-7_15},
	abstract = {In this paper, we introduce a design and implementation of the free agent threads for OpenMP. These threads increase the malleability of the OpenMP programming model, offering resource managers and runtime systems flexibility to manage threads and resources efficiently. We demonstrate how free agent threads can address load imbalances problems at the OpenMP level and at an MPI level or higher. We use two mini-apps extracted from two real HPC applications and representative of real-world codes to demonstrate this. We conclude that more malleability in thread management is necessary, and free agents can be regarded as a practical starting point to increase malleability in thread management.},
	urldate = {2025-07-01},
	booktitle = {{OpenMP}: {Enabling} {Massive} {Node}-{Level} {Parallelism}: 17th {International} {Workshop} on {OpenMP}, {IWOMP} 2021, {Bristol}, {UK}, {September} 14–16, 2021, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Lopez, Victor and Criado, Joel and Peñacoba, Raúl and Ferrer, Roger and Teruel, Xavier and Garcia-Gasulla, Marta},
	month = sep,
	year = {2021},
	pages = {211--225},
}

@inproceedings{martin_flex-mpi_2013,
	title = {{FLEX}-{MPI}: an {MPI} {Extension} for {Supporting} {Dynamic} {Load} {Balancing} on {Heterogeneous} {Non}-dedicated {Systems}},
	isbn = {978-3-642-40046-9},
	booktitle = {Euro-{Par} {Parallel} {Processing}},
	author = {Martín, Gonzalo and Marinescu, Maria-Cristina and Singh, David E. and Carretero, Jesús},
	month = aug,
	year = {2013},
	pages = {138--149},
}